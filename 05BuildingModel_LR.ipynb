{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the various models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import NullFormatter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.ticker as ticker\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import f1_score, jaccard_similarity_score\n",
    "\n",
    "%matplotlib inline\n",
    "Feature=pd.read_csv('Feature')\n",
    "df = pd.read_csv('loan_train.csv')\n",
    "X=np.load('X.npy')\n",
    "y = df['loan_status'].values #labels\n",
    "#X= preprocessing.StandardScaler().fit(X).transform(X)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Jaccard Index is: 0.8\n",
      "The Log Loss is: 0.47469572648067415\n"
     ]
    }
   ],
   "source": [
    "#model\n",
    "X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.1, random_state=4)\n",
    "\n",
    "LR = LogisticRegression(C=0.18, solver='saga').fit(X_train,y_train)\n",
    "yhat = LR.predict(X_test)\n",
    "yhat_prob = LR.predict_proba(X_test)\n",
    "\n",
    "print('The Jaccard Index is:',jaccard_similarity_score(y_test, yhat))\n",
    "print('The Log Loss is:',log_loss(y_test, yhat_prob))\n",
    "\n",
    "#For use in test \n",
    "pd.DataFrame(X_train).to_csv('Xtrain_LR',index=None)\n",
    "pd.DataFrame(y_train).to_csv('ytrain_LR',index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Jaccard Index is: 0.8\n",
      "The Log Loss is: 0.48911154805314305\n",
      "The Jaccard Index is: 0.8\n",
      "The Log Loss is: 0.48911154805314305\n",
      "The Jaccard Index is: 0.8\n",
      "The Log Loss is: 0.48911154805314305\n",
      "The Jaccard Index is: 0.8\n",
      "The Log Loss is: 0.48911154805314305\n"
     ]
    }
   ],
   "source": [
    "#Finding optimal test_size, .1\n",
    "for size in range(1,5):\n",
    "    X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=.1, random_state=4)\n",
    "    LR = LogisticRegression(C=0.18, solver='liblinear').fit(X_train,y_train)\n",
    "    yhat = LR.predict(X_test)\n",
    "    yhat_prob = LR.predict_proba(X_test)\n",
    "\n",
    "    print('The Jaccard Index is:',jaccard_similarity_score(y_test, yhat))\n",
    "    print('The Log Loss is:',log_loss(y_test, yhat_prob))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Jaccard Index is: 0.7714285714285715\n",
      "The Log Loss is: 0.5674864788622827\n",
      "The Jaccard Index is: 0.7714285714285715\n",
      "The Log Loss is: 0.5324335001425637\n",
      "The Jaccard Index is: 0.7714285714285715\n",
      "The Log Loss is: 0.5163314912635928\n",
      "The Jaccard Index is: 0.7714285714285715\n",
      "The Log Loss is: 0.5073270290138204\n",
      "The Jaccard Index is: 0.7714285714285715\n",
      "The Log Loss is: 0.5017324316144246\n",
      "The Jaccard Index is: 0.7714285714285715\n",
      "The Log Loss is: 0.49802497245512994\n",
      "The Jaccard Index is: 0.7714285714285715\n",
      "The Log Loss is: 0.49546183685497674\n",
      "The Jaccard Index is: 0.7714285714285715\n",
      "The Log Loss is: 0.493639456906383\n",
      "The Jaccard Index is: 0.8\n",
      "The Log Loss is: 0.49232224564108185\n",
      "The Jaccard Index is: 0.8\n",
      "The Log Loss is: 0.49135730618263196\n",
      "The Jaccard Index is: 0.8\n",
      "The Log Loss is: 0.49064833513344447\n",
      "The Jaccard Index is: 0.8\n",
      "The Log Loss is: 0.49012837787528024\n",
      "The Jaccard Index is: 0.8\n",
      "The Log Loss is: 0.48975224511949306\n",
      "The Jaccard Index is: 0.8\n",
      "The Log Loss is: 0.4894859927847616\n",
      "The Jaccard Index is: 0.8\n",
      "The Log Loss is: 0.4893050730871609\n",
      "The Jaccard Index is: 0.8\n",
      "The Log Loss is: 0.48919112518403296\n",
      "The Jaccard Index is: 0.8\n",
      "The Log Loss is: 0.4891301989464734\n",
      "The Jaccard Index is: 0.8\n",
      "The Log Loss is: 0.48911154805314305\n",
      "The Jaccard Index is: 0.8\n",
      "The Log Loss is: 0.48912679282646965\n"
     ]
    }
   ],
   "source": [
    "#in searching for optimal values for C we find that c=.18\n",
    "for c in range(1,20):\n",
    "    X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=.1, random_state=4)\n",
    "    LR = LogisticRegression(C=c/100, solver='liblinear').fit(X_train,y_train)\n",
    "    yhat = LR.predict(X_test)\n",
    "    yhat_prob = LR.predict_proba(X_test)\n",
    "\n",
    "    print('The Jaccard Index is:',jaccard_similarity_score(y_test, yhat))\n",
    "    print('The Log Loss is:',log_loss(y_test, yhat_prob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Jaccard Index is: 0.8\n",
      "The Log Loss is: 0.4746944086343245\n",
      "The Jaccard Index is: 0.8\n",
      "The Log Loss is: 0.47469472353890024\n",
      "The Jaccard Index is: 0.8\n",
      "The Log Loss is: 0.48911154805314305\n",
      "The Jaccard Index is: 0.8\n",
      "The Log Loss is: 0.4746939278705231\n",
      "The Jaccard Index is: 0.8\n",
      "The Log Loss is: 0.4746949366612874\n"
     ]
    }
   ],
   "source": [
    "#in searching for optimal values for solvers, we find liblinear is indeed the worst and choose newton-cg\n",
    "for Solver in ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']:\n",
    "    X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=.1, random_state=4)\n",
    "    LR = LogisticRegression(C=.18, solver=Solver).fit(X_train,y_train)\n",
    "    yhat = LR.predict(X_test)\n",
    "    yhat_prob = LR.predict_proba(X_test)\n",
    "\n",
    "    print('The Jaccard Index is:',jaccard_similarity_score(y_test, yhat))\n",
    "    print('The Log Loss is:',log_loss(y_test, yhat_prob))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We see here that the optimal test size is .1, the c-value .18, and solver is saga, due to logloss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
